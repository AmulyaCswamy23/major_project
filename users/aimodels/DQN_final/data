{
    "policy_class": {
        ":type:": "<class 'abc.ABCMeta'>",
        ":serialized:": "gAWVMAAAAAAAAACMHnN0YWJsZV9iYXNlbGluZXMzLmRxbi5wb2xpY2llc5SMCURRTlBvbGljeZSTlC4=",
        "__module__": "stable_baselines3.dqn.policies",
        "__firstlineno__": 88,
        "__annotations__": "{'q_net': <class 'stable_baselines3.dqn.policies.QNetwork'>, 'q_net_target': <class 'stable_baselines3.dqn.policies.QNetwork'>}",
        "__doc__": "\nPolicy class with Q-Value Net and target net for DQN\n\n:param observation_space: Observation space\n:param action_space: Action space\n:param lr_schedule: Learning rate schedule (could be constant)\n:param net_arch: The specification of the policy and value networks.\n:param activation_fn: Activation function\n:param features_extractor_class: Features extractor to use.\n:param features_extractor_kwargs: Keyword arguments\n    to pass to the features extractor.\n:param normalize_images: Whether to normalize images or not,\n     dividing by 255.0 (True by default)\n:param optimizer_class: The optimizer to use,\n    ``th.optim.Adam`` by default\n:param optimizer_kwargs: Additional keyword arguments,\n    excluding the learning rate, to pass to the optimizer\n",
        "__init__": "<function DQNPolicy.__init__ at 0x0000021648F0BB00>",
        "_build": "<function DQNPolicy._build at 0x0000021648F0BBA0>",
        "make_q_net": "<function DQNPolicy.make_q_net at 0x0000021648F0BC40>",
        "forward": "<function DQNPolicy.forward at 0x0000021648F0BCE0>",
        "_predict": "<function DQNPolicy._predict at 0x0000021648F0BD80>",
        "_get_constructor_parameters": "<function DQNPolicy._get_constructor_parameters at 0x0000021648F0BE20>",
        "set_training_mode": "<function DQNPolicy.set_training_mode at 0x0000021648F0BEC0>",
        "__static_attributes__": [
            "activation_fn",
            "net_arch",
            "net_args",
            "optimizer",
            "q_net",
            "q_net_target",
            "training"
        ],
        "__abstractmethods__": "frozenset()",
        "_abc_impl": "<_abc._abc_data object at 0x0000021648F18A00>"
    },
    "verbose": 1,
    "policy_kwargs": {},
    "num_timesteps": 50000,
    "_total_timesteps": 50000,
    "_num_timesteps_at_start": 0,
    "seed": null,
    "action_noise": null,
    "start_time": 1762063531218986600,
    "learning_rate": 0.001,
    "tensorboard_log": null,
    "_last_obs": {
        ":type:": "<class 'numpy.ndarray'>",
        ":serialized:": "gAWVigAAAAAAAACME251bXB5Ll9jb3JlLm51bWVyaWOUjAtfZnJvbWJ1ZmZlcpSTlCiWFAAAAAAAAAAjW/8+PeTYPQAAgD9aLus++m72PpSMBW51bXB5lIwFZHR5cGWUk5SMAmY0lImIh5RSlChLA4wBPJROTk5K/////0r/////SwB0lGJLAUsFhpSMAUOUdJRSlC4="
    },
    "_last_episode_starts": {
        ":type:": "<class 'numpy.ndarray'>",
        ":serialized:": "gAWVdQAAAAAAAACME251bXB5Ll9jb3JlLm51bWVyaWOUjAtfZnJvbWJ1ZmZlcpSTlCiWAQAAAAAAAAABlIwFbnVtcHmUjAVkdHlwZZSTlIwCYjGUiYiHlFKUKEsDjAF8lE5OTkr/////Sv////9LAHSUYksBhZSMAUOUdJRSlC4="
    },
    "_last_original_obs": {
        ":type:": "<class 'numpy.ndarray'>",
        ":serialized:": "gAWVigAAAAAAAACME251bXB5Ll9jb3JlLm51bWVyaWOUjAtfZnJvbWJ1ZmZlcpSTlCiWFAAAAAAAAAAAAIA/IvIZPgAAAADn9po+AAAAAJSMBW51bXB5lIwFZHR5cGWUk5SMAmY0lImIh5RSlChLA4wBPJROTk5K/////0r/////SwB0lGJLAUsFhpSMAUOUdJRSlC4="
    },
    "_episode_num": 2500,
    "use_sde": false,
    "sde_sample_freq": -1,
    "_current_progress_remaining": 0.0,
    "_stats_window_size": 100,
    "ep_info_buffer": {
        ":type:": "<class 'collections.deque'>",
        ":serialized:": "gAWVIAAAAAAAAACMC2NvbGxlY3Rpb25zlIwFZGVxdWWUk5QpS2SGlFKULg=="
    },
    "ep_success_buffer": {
        ":type:": "<class 'collections.deque'>",
        ":serialized:": "gAWVIAAAAAAAAACMC2NvbGxlY3Rpb25zlIwFZGVxdWWUk5QpS2SGlFKULg=="
    },
    "_n_updates": 12475,
    "observation_space": {
        ":type:": "<class 'gymnasium.spaces.box.Box'>",
        ":serialized:": "gAWVsQEAAAAAAACMFGd5bW5hc2l1bS5zcGFjZXMuYm94lIwDQm94lJOUKYGUfZQojAVkdHlwZZSMBW51bXB5lIwFZHR5cGWUk5SMAmY0lImIh5RSlChLA4wBPJROTk5K/////0r/////SwB0lGKMBl9zaGFwZZRLBYWUjANsb3eUjBNudW1weS5fY29yZS5udW1lcmljlIwLX2Zyb21idWZmZXKUk5QolhQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACUaAtLBYWUjAFDlHSUUpSMDWJvdW5kZWRfYmVsb3eUaBMolgUAAAAAAAAAAQEBAQGUaAiMAmIxlImIh5RSlChLA4wBfJROTk5K/////0r/////SwB0lGJLBYWUaBZ0lFKUjARoaWdolGgTKJYUAAAAAAAAAAAAgD8AAIA/AACAPwAAgD8AAIA/lGgLSwWFlGgWdJRSlIwNYm91bmRlZF9hYm92ZZRoEyiWBQAAAAAAAAABAQEBAZRoHUsFhZRoFnSUUpSMCGxvd19yZXBylIwDMC4wlIwJaGlnaF9yZXBylIwDMS4wlIwKX25wX3JhbmRvbZROdWIu",
        "dtype": "float32",
        "_shape": [
            5
        ],
        "low": "[0. 0. 0. 0. 0.]",
        "bounded_below": "[ True  True  True  True  True]",
        "high": "[1. 1. 1. 1. 1.]",
        "bounded_above": "[ True  True  True  True  True]",
        "low_repr": "0.0",
        "high_repr": "1.0",
        "_np_random": null
    },
    "action_space": {
        ":type:": "<class 'gymnasium.spaces.discrete.Discrete'>",
        ":serialized:": "gAWVpgIAAAAAAACMGWd5bW5hc2l1bS5zcGFjZXMuZGlzY3JldGWUjAhEaXNjcmV0ZZSTlCmBlH2UKIwBbpSMFm51bXB5Ll9jb3JlLm11bHRpYXJyYXmUjAZzY2FsYXKUk5SMBW51bXB5lIwFZHR5cGWUk5SMAmk4lImIh5RSlChLA4wBPJROTk5K/////0r/////SwB0lGJDCAMAAAAAAAAAlIaUUpSMBXN0YXJ0lGgIaA5DCAAAAAAAAAAAlIaUUpSMBl9zaGFwZZQpjAVkdHlwZZRoDowKX25wX3JhbmRvbZSMFG51bXB5LnJhbmRvbS5fcGlja2xllIwQX19nZW5lcmF0b3JfY3RvcpSTlGgbjBRfX2JpdF9nZW5lcmF0b3JfY3RvcpSTlIwTbnVtcHkucmFuZG9tLl9wY2c2NJSMBVBDRzY0lJOUhZRSlH2UKIwNYml0X2dlbmVyYXRvcpSMBVBDRzY0lIwFc3RhdGWUfZQoaCiKEDIRD5DsoRdMbo9TQ8C6XVqMA2luY5SKEBmszM2TXriAwQpsMFimGyh1jApoYXNfdWludDMylEsAjAh1aW50ZWdlcpRKWXB5cnWMGm51bXB5LnJhbmRvbS5iaXRfZ2VuZXJhdG9ylIwbX19weXhfdW5waWNrbGVfU2VlZFNlcXVlbmNllJOUaC2MDFNlZWRTZXF1ZW5jZZSTlEoiouoDToeUUpQoihC4YfjHJ2E5wt4VKbYhzVxaSwCME251bXB5Ll9jb3JlLm51bWVyaWOUjAtfZnJvbWJ1ZmZlcpSTlCiWEAAAAAAAAADL9we01OOPTD+nqGeoTnTLlGgLjAJ1NJSJiIeUUpQoSwNoD05OTkr/////Sv////9LAHSUYksEhZSMAUOUdJRSlEsEKXSUYoaUYoWUUpR1Yi4=",
        "n": "3",
        "start": "0",
        "_shape": [],
        "dtype": "int64",
        "_np_random": "Generator(PCG64)"
    },
    "n_envs": 1,
    "buffer_size": 10000,
    "batch_size": 32,
    "learning_starts": 100,
    "tau": 1.0,
    "gamma": 0.99,
    "gradient_steps": 1,
    "optimize_memory_usage": false,
    "replay_buffer_class": {
        ":type:": "<class 'abc.ABCMeta'>",
        ":serialized:": "gAWVNQAAAAAAAACMIHN0YWJsZV9iYXNlbGluZXMzLmNvbW1vbi5idWZmZXJzlIwMUmVwbGF5QnVmZmVylJOULg==",
        "__module__": "stable_baselines3.common.buffers",
        "__firstlineno__": 158,
        "__annotations__": "{'observations': <class 'numpy.ndarray'>, 'next_observations': <class 'numpy.ndarray'>, 'actions': <class 'numpy.ndarray'>, 'rewards': <class 'numpy.ndarray'>, 'dones': <class 'numpy.ndarray'>, 'timeouts': <class 'numpy.ndarray'>}",
        "__doc__": "\nReplay buffer used in off-policy algorithms like SAC/TD3.\n\n:param buffer_size: Max number of element in the buffer\n:param observation_space: Observation space\n:param action_space: Action space\n:param device: PyTorch device\n:param n_envs: Number of parallel environments\n:param optimize_memory_usage: Enable a memory efficient variant\n    of the replay buffer which reduces by almost a factor two the memory used,\n    at a cost of more complexity.\n    See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n    and https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n    Cannot be used in combination with handle_timeout_termination.\n:param handle_timeout_termination: Handle timeout termination (due to timelimit)\n    separately and treat the task as infinite horizon task.\n    https://github.com/DLR-RM/stable-baselines3/issues/284\n",
        "__init__": "<function ReplayBuffer.__init__ at 0x0000021648E98040>",
        "add": "<function ReplayBuffer.add at 0x0000021648E98180>",
        "sample": "<function ReplayBuffer.sample at 0x0000021648E98220>",
        "_get_samples": "<function ReplayBuffer._get_samples at 0x0000021648E982C0>",
        "_maybe_cast_dtype": "<staticmethod(<function ReplayBuffer._maybe_cast_dtype at 0x0000021648E98360>)>",
        "__static_attributes__": [
            "actions",
            "buffer_size",
            "dones",
            "full",
            "handle_timeout_termination",
            "next_observations",
            "observations",
            "optimize_memory_usage",
            "pos",
            "rewards",
            "timeouts"
        ],
        "__abstractmethods__": "frozenset()",
        "_abc_impl": "<_abc._abc_data object at 0x0000021648E8A4C0>"
    },
    "replay_buffer_kwargs": {},
    "n_steps": 1,
    "train_freq": {
        ":type:": "<class 'stable_baselines3.common.type_aliases.TrainFreq'>",
        ":serialized:": "gAWVYQAAAAAAAACMJXN0YWJsZV9iYXNlbGluZXMzLmNvbW1vbi50eXBlX2FsaWFzZXOUjAlUcmFpbkZyZXGUk5RLBGgAjBJUcmFpbkZyZXF1ZW5jeVVuaXSUk5SMBHN0ZXCUhZRSlIaUgZQu"
    },
    "use_sde_at_warmup": false,
    "exploration_initial_eps": 1.0,
    "exploration_final_eps": 0.05,
    "exploration_fraction": 0.1,
    "target_update_interval": 10000,
    "_n_calls": 50000,
    "max_grad_norm": 10,
    "exploration_rate": 0.05,
    "lr_schedule": {
        ":type:": "<class 'stable_baselines3.common.utils.FloatSchedule'>",
        ":serialized:": "gAWVeQAAAAAAAACMHnN0YWJsZV9iYXNlbGluZXMzLmNvbW1vbi51dGlsc5SMDUZsb2F0U2NoZWR1bGWUk5QpgZR9lIwOdmFsdWVfc2NoZWR1bGWUaACMEENvbnN0YW50U2NoZWR1bGWUk5QpgZR9lIwDdmFslEc/UGJN0vGp/HNic2Iu",
        "value_schedule": "ConstantSchedule(val=0.001)"
    },
    "batch_norm_stats": [],
    "batch_norm_stats_target": [],
    "exploration_schedule": {
        ":type:": "<class 'stable_baselines3.common.utils.LinearSchedule'>",
        ":serialized:": "gAWVdQAAAAAAAACMHnN0YWJsZV9iYXNlbGluZXMzLmNvbW1vbi51dGlsc5SMDkxpbmVhclNjaGVkdWxllJOUKYGUfZQojAVzdGFydJRHP/AAAAAAAACMA2VuZJRHP6mZmZmZmZqMDGVuZF9mcmFjdGlvbpRHP7mZmZmZmZp1Yi4=",
        "start": 1.0,
        "end": 0.05,
        "end_fraction": 0.1
    }
}